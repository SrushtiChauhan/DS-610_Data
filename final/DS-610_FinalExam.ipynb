{"cells":[{"cell_type":"markdown","source":["<pre><b>Q1 Python </b>\nThe following code separates given text into smaller texts based on\na desired width. Write a python function that replicates the same\nbehavior without using the textwrap.wrap function. Run your code\nwith an example and show the output. Explain your code and logic\nwith comments.\n\nExample of the desired action with the library `textwrap`:\nInput:\nimport textwrap\ntxt = 'AaaaaaaaaaaaAaaaaaaaaaaaAaaaaaaaaaaaAaaaaaaaaaaaAaaaaaaaaaaa'\ntextwrap.wrap(txt, 12)\nOutput:\n['Aaaaaaaaaaaa', 'Aaaaaaaaaaaa', 'Aaaaaaaaaaaa',\n'Aaaaaaaaaaaa', 'Aaaaaaaaaaaa']</pre>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"78f4ef42-806a-45b5-858c-628c17b02628"}}},{"cell_type":"code","source":["import textwrap\ntxt = 'AaaaaaaaaaaaAaaaaaaaaaaaAaaaaaaaaaaaAaaaaaaaaaaaAaaaaaaaaaaa'\ntextwrap.wrap(txt, 12)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3f62c94-f4cc-4e9b-97bb-f1589d744211"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[4]: [&#39;Aaaaaaaaaaaa&#39;,\n &#39;Aaaaaaaaaaaa&#39;,\n &#39;Aaaaaaaaaaaa&#39;,\n &#39;Aaaaaaaaaaaa&#39;,\n &#39;Aaaaaaaaaaaa&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[4]: [&#39;Aaaaaaaaaaaa&#39;,\n &#39;Aaaaaaaaaaaa&#39;,\n &#39;Aaaaaaaaaaaa&#39;,\n &#39;Aaaaaaaaaaaa&#39;,\n &#39;Aaaaaaaaaaaa&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import re\n\nmy_str = 'AaaaaaaaaaaaAaaaaaaaaaaaAaaaaaaaaaaaAaaaaaaaaaaaAaaaaaaaaaaa'\n\nmy_list = re.findall('[a-zA-Z][^A-Z]*', my_str)\n\nprint(my_list)  # üëâÔ∏è ['Aaaaaaaaaaaa', 'Aaaaaaaaaaaa', 'Aaaaaaaaaaaa', 'Aaaaaaaaaaaa', 'Aaaaaaaaaaaa']\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec60480d-0395-4850-8725-5297484c5bcd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[&#39;Aaaaaaaaaaaa&#39;, &#39;Aaaaaaaaaaaa&#39;, &#39;Aaaaaaaaaaaa&#39;, &#39;Aaaaaaaaaaaa&#39;, &#39;Aaaaaaaaaaaa&#39;]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;Aaaaaaaaaaaa&#39;, &#39;Aaaaaaaaaaaa&#39;, &#39;Aaaaaaaaaaaa&#39;, &#39;Aaaaaaaaaaaa&#39;, &#39;Aaaaaaaaaaaa&#39;]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["<pre><b>Q2 Knowledge </b>\nExplain what Jobs, Stages and Tasks are and why Apache Spark splits up an application into Jobs, Stages and Tasks.\nExplain what Transformations and Actions are. Give your reasoning of why these structures are built in this way. Give examples for both types.</pre>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0200f148-8d5a-40a5-b1e0-8d07229b69a4"}}},{"cell_type":"markdown","source":["<pre>\nOverview of Spark Stages <br/>\nSpark stages are the physical unit of execution for the computation of multiple tasks. The Spark stages are controlled by the Directed Acyclic Graph(DAG) for any data processing and transformations on the resilient distributed datasets(RDD). There are mainly two stages associated with the Spark frameworks such as, ShuffleMapStage and ResultStage. The Shuffle MapStage is the intermediate phase for the tasks which prepares data for subsequent stages, whereas resultStage is a final step to the spark function for the particular set of tasks in the spark job. ResultSet is associated with the initialization of parameter, counters and registry values in Spark.\n\nThe meaning of DAG is as follows:\n\nDirected: All the nodes are connected to one another creating an acyclic graph. The sequence of this is determined by the actions called on the RDD.\nAcyclic: The nodes are not connected as a cyclic loop i.e. if an action or a transformation was once done cannot be reverted back to its original value.\nGraph: The entire pattern formed by the edges and vertices arranged together in a specific pattern is called a graph. Vertices are nothing but the RDD‚Äôs and the edges are the actions called on the RDD.\nDAGScheduler is the one that divides the stages into a number of tasks. The DAGScheduler then passes the stage information to the cluster manager(YARN/Spark standalone) which triggers the task scheduler to run the tasks. Spark driver converts the logical plan to a physical execution plan. Spark jobs are executed in the pipelining method where all the transformation tasks are combined into a single stage.\n\nTransformations <br/>\nThere are 2 kinds of transformations which take place:\n\n1. Narrow Transformations: These are transformations that do not require the process of shuffling. These actions can be executed in a single stage.\n\nExample: map() and filter()\n\n2. Wide Transformations: These are transformations that require shuffling across various partitions. Hence it requires different stages to be created for communication across different partitions.\n\nExample: ReduceByKey\n\nLet‚Äôs take an example for a better understanding of how this works.\n\nExample: In this example, we will see how a simple word count works using Spark DAGScheduler.\n\nval data = sc.textFile(‚Äúdata.txt‚Äù)\nResult: data: org.apache.spark.rdd.RDD[String] = data.txt MapPartitionsRDD[46] at textFile at <console>:24\n\nFirst, a textFile operation is performed to read the given input text file from the HDFS location.\n\ndata.flatMap(_.split(‚Äù ‚Äú)).map(i=>(i,1)).reduceByKey(_ + _).collect\nResult: res21: Array[(String, Int)] = Array()\n\nNext, a flatMap operation is performed to split the lines in the entire input file into different words. Then a map operation is done to form (key, value) pairs like (word,1)  for each of the words. And the reduceByKey function is called to find the sum of counts for each word. Finally, the collective action will give the end result by collecting all the data.\n\n<img src=\"https://github.com/SrushtiChauhan/DS-610_Data/blob/main/final/1.PNG?raw=true\" alt=\"x\">\n  \nDuring this program, 2 stages are created by Spark because a transformation is performed here. While transformation operation is done, shuffling needs to be performed because the data needs to be shuffled between 2 or more different partitions. Hence for this, a stage is created and then another single stage for the transformation task is created.\n  \n<img src=\"https://github.com/SrushtiChauhan/DS-610_Data/blob/main/final/2.PNG?raw=true\" alt=\"x\">\n  \nAlso internally these stages will be divided into tasks. In this example, each stage is divided into 2 tasks since there are 2 partitions that exist. Each partition runs an individual task.\n\nTypes of Spark Stages <br/>\nHere are the two types described in detail.\n\n1. ShuffleMapStage\nThis is basically an intermediate stage in the process of DAG execution. The output of this stage is used as the input for further stage(s). The output of this is in the form of map output files which can be later used by reducing task. A ShuffleMapStage is considered ready when its all map outputs are available. Sometimes the output locations can be missing in cases where the partitions are either lost or not available.\n\nThis stage may contain many pipeline operations such as map() and filter() before the execution of shuffling. Internal registries outputLocs and _numAvailableOutputs are used by ShuffleMapStage to track the number of shuffle map outputs. A single ShuffleMapStage can be used commonly across various jobs.\n\n2. ResultStage\nAs the name itself suggests, this is the final stage in a Spark job which performs an operation on one or more partitions of an RDD to calculate its result. Initialization of internal registries and counters is done by the ResultStage.\n\nThe DAGScheduler submits missing tasks if any to the ResultStage for computation. For computation, it requires various mandatory parameters such as stageId, stageAttempId, the broadcast variable of the serialized task, partition, preferred TaskLocations, outputId, some local properties, TaskMetrics of that particular stage. Some of the optional parameters required are Job Id, Application Id, and Application attempt Id.\n\nAdvantages of Spark Stages <br/>\nBelow are the different advantages of Spark Stages:\n\n1. Dynamic allocation of executors <br/>\nBy seeing the Spark Job Event Timeline we can see that the allocation of executors is done dynamically. This means the executors are called from the cluster depending on the workload during the course of task execution. It is then released back to the cluster as soon as its job is done. This saves the resource allocation memory and allows the other applications running on the same cluster to reuse the executors. Hence the overall cluster utilization will increase and be optimal.\n  \n2. Caching <br/>\nRDD‚Äôs are cached during the operations performed on them on each stage and stored in the memory. This is helpful in saving computational time when the end result requires the same RDD‚Äôs to be read again from HDFS.\n\n3. Parallel execution <br/>\nSpark jobs that are independent of each other are executed in parallel unless and until there is a shuffling required or the input of one stage is dependent on its previous output.\n\n4. DAG Visualization <br/>\nThis is very helpful in cases of complex computations where a lot of operations and their dependencies are involved. Seeing this DAG Visualization, one can easily trace the flow and identify the performance blockages. Also, one can see each of the tasks run by each stage by clicking on the stages shown in this visualization. In this expanded view, all the details of the RDD‚Äôs which belong to this stage are shown.\n\n5. Fault tolerance <br/>\nDue to the caching operation performed on RDD‚Äôs, DAG will have a record of each action performed on them. Hence suppose in any case an RDD is lost, it can easily be retrieved with the help of DAG. Cluster manager can be used to identify the partition at which it was lost and the same RDD can be placed again at the same partition for data loss recovery.\n\nDue to the above-mentioned benefits, Apache Spark is being widely used instead of the previously used MapReduce. It is nothing but an extended version of the MapReduce. Since MapReduce required the data to be read from and written to the HDFS multiple times, Spark was introduced which does these actions in its in-memory.\n\nConclusion <br/>\nHence we can conclude that it is more efficient because of their in-memory computation, increased processing speed even for iterative processing.\n</pre>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c2cdde7-b838-4331-910c-4872d90fee86"}}},{"cell_type":"markdown","source":["<pre><b>Q3 Knowledge </b>\nExplain what narrow and wide Transformations are and how they differ from each other. Give examples for both types.</pre>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12cc9023-6d83-4ccc-ab74-618ee5de50c9"}}},{"cell_type":"markdown","source":["Transformation in Spark <br/>\nSpark Transformation is a function that produces new RDDfrom the existing RDDs. It takes RDD as input and produces one or more RDD as output. Each time it creates new RDD when we apply any transformation. As RDDs are immutable in nature, so input RDDs, cannot be changed.\nAn RDD lineage, built by Applying transformation built with the entire parent RDDs of the final RDD(s). In other words, it is also known as RDD operator graph or RDD dependency graph. It is a logical execution plan i.e., it is Directed Acyclic Graph (DAG) of the entire parent RDDs of RDD.\n\nTransformations are lazy in nature i.e., they get execute when we call an action. They are not executed immediately. Two most basic type of transformations is a map(), filter().\n\nResultant RDD is always dissimilar from its parent RDD. It can be smaller (e.g. filter, count, distinct, sample), bigger (e.g. flatMap(), union(), Cartesian()) or the same size (e.g. map).\n\nNow, let‚Äôs focus on the question, there are fundamentally two types of transformations:\n\n1. Narrow transformation ‚Äì <br/>\nWhile talking about Narrow transformation, all the elements which are required to compute the records in single partition reside in the single partition of parent RDD. To calculate the result, a limited subset of partition is used. This Transformation are the result of map(), filter().\n\n2. Wide Transformations ‚Äì  <br/>\nWide transformation means all the elements that are required to compute the records in the single partition may live in many partitions of parent RDD. Partitions may reside in many different partitions of parent RDD. This Transformation is a result of groupbyKey() and reducebyKey()."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d18cbf1c-5ce3-4671-948d-c1e3600b98f7"}}},{"cell_type":"markdown","source":["<pre><b>Q4 Knowledge </b>\nWhich part of the Spark architecture is responsible for deciding how to subdivide the larger dataset into at 128 MB chunks?</pre>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d423d6a0-eeb4-4a56-84c0-0e97708d41bb"}}},{"cell_type":"markdown","source":["The Spark Executors\n\nThe core responsibility of a Spark executor is to take the assigned tasks, run them, and report back their success or failure state and results. Each Spark application has its own separate executor processes."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21990a94-8e8c-4f60-b228-76ea8ae84be6"}}},{"cell_type":"markdown","source":["<pre><b>Q5 Knowledge </b>\nWhat term identifies the smallest unit of work in a Spark application?</pre>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0cfe3caa-d48b-4d6b-a505-7d94dcb171f6"}}},{"cell_type":"markdown","source":["Application - A user program built on Spark using its APIs. It consists of a driver program and executors on the cluster.\n\nJob - A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g., save(), collect()). During interactive sessions with Spark shells, the driver converts your Spark application into one or more Spark jobs. It then transforms each job into a DAG. This, in essence, is Spark‚Äôs execution plan, where each node within a DAG could be a single or multiple Spark stages.\n\nStage - Each job gets divided into smaller sets of tasks called stages that depend on each other. As part of the DAG nodes, stages are created based on what operations can be performed serially or in parallel. Not all Spark operations can happen in a single stage, so they may be divided into multiple stages. Often stages are delineated on the operator‚Äôs computation boundaries, where they dictate data transfer among Spark executors.\n\nTask - A single unit of work or execution that will be sent to a Spark executor. Each stage is comprised of Spark tasks (a unit of execution), which are then federated across each Spark executor; each task maps to a single core and works on a single partition of data. As such, an executor with 16 cores can have 16 or more tasks working on 16 or more partitions in parallel, making the execution of Spark‚Äôs tasks exceedingly parallel!\n\n <br/>\n<img src=\"https://i.stack.imgur.com/zxbzi.png?raw=true\" alt=\"x\">"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"150ce5fe-1968-4533-99d7-28f27524c20b"}}},{"cell_type":"markdown","source":["<pre><b>Q6 Knowledge </b>\nWhat term identifies the environment in which a task is executed?</pre>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90f249e5-264b-4b0f-8955-f58b1a0cf2f6"}}},{"cell_type":"markdown","source":["<pre>\n1. Spark Context\nSparkContext is the main entry point to spark core. It allows us to access further functionalities of spark. This helps to establish a connection to spark execution environment. It provides access to spark cluster even with a resource manager. Sparkcontext act as master of spark application.\n\nIt offers various functions. Such as:\n\nGetting the current status of spark application\nCanceling the job\nCanceling the Stage\nRunning job synchronously\nRunning job asynchronously\nAccessing persistent RDD\nUn-persisting RDD\nProgrammable dynamic allocation\n\n2. Spark Shell\nApache spark provides interactive spark shell which allows us to run applications on. It helps in processing a large amount of data because it can read many types of data. Run/test of our application code interactively is possible by using spark shell.\n\n3. Spark Application\nEven when there is no job running, spark application can have processes running on its behalf. It is a self-contained computation that runs user-supplied code to compute a result.\n\n4. Task\nIt is a unit of work, which we sent to the executor. Every stage has some task, one task per partition.\n\n5. Job\nIt parallels computation consisting of multiple tasks.\n\n6. Stages\nEach job is divided into small sets of tasks which are known as stages.\n</pre>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9e7f3c3-b0ba-4e48-ba6c-2887fcf52d88"}}},{"cell_type":"markdown","source":["<pre><b>Q7 Knowledge </b>\nWhich two parts of the Spark architecture are run inside of a Java\nVirtual Machine (JVM)?</pre>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c60ffadc-d341-44fc-b90b-daf4bf83a972"}}},{"cell_type":"markdown","source":["All Spark components, including the Driver, Master, and Executor processes, run in Java virtual machines (JVMs)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b385ea6d-9964-4244-b2a8-10c8160acf34"}}},{"cell_type":"markdown","source":["<pre><b>Q8 Knowledge </b>\nExplain the roles of the Spark Driver and why there is a need to such component in the Spark architecture.</pre>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a27de90b-f9c9-4265-8167-81e6dbdb8872"}}},{"cell_type":"markdown","source":["Spark Driver works in conjunction with the Cluster Manager to control the execution of various other jobs. The cluster Manager does the task of allocating resources for the job. Once the job has been broken down into smaller jobs, which are then distributed to worker nodes, SparkDriver will control the execution."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da191ee9-cd49-4b81-95c5-bb088eb7dcdd"}}},{"cell_type":"markdown","source":["<pre><b>Q9 Knowledge </b>\nTrue or False, in a job with multiple stages, the execution of the secondary stages can be delayed by a single, slow-running task in the previous stage?</pre>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b94f449-0904-4bf7-b9c0-727a1cd54c09"}}},{"cell_type":"markdown","source":["False"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42dbab6e-be7b-4839-9a52-4fe23a4262a3"}}},{"cell_type":"markdown","source":["<pre><b>Q10 Knowledge </b>\nWhich part of the Spark architecture is responsible for deciding which task processes which piece of data?</pre>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96e9620b-35c2-4b57-8ca2-512b12f4f191"}}},{"cell_type":"markdown","source":["The Spark Executors <br/>\nThe core responsibility of a Spark executor is to take the assigned tasks, run them, and report back their success or failure state and results. Each Spark application has its own separate executor processes."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"29ed28f6-624b-4789-b1b4-ed60b68cecfc"}}},{"cell_type":"markdown","source":["<pre><b>Q11 Spark API </b>\nUsing sales.csv from the attached file:\nWrite a Spark code to return the sum, average, minimum, maximum value of sales amount by ordermonthyear & productcategory field.\nSort the results in descending order by ordermonthyear. Submit the screenshot of your code view showing query and the first 5 rows of the result.</pre>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"abb480a0-7419-4a19-b8eb-fc5768341d7f"}}},{"cell_type":"code","source":["# spark is from the previous example\nsc = spark.sparkContext\n\n# A CSV dataset is pointed to by path.\n# The path can be either a single CSV file or a directory of CSV files\npath = \"/FileStore/tables/sales.csv\"\n\ndf = spark.read.option(\"header\", \"true\").csv(path)\n#df.show()\n# display(df)\ndf.select(\"SaleAmount\",\"OrderMonthYear\",\"ProductCategory\").show(2)\n\n# Register the DataFrame as a SQL temporary view\ndf.createOrReplaceTempView(\"Sales\")\n\nsqlDF = spark.sql(\"SELECT OrderMonthYear,ProductCategory,sum(SaleAmount),avg(SaleAmount),min(SaleAmount),max(SaleAmount) FROM Sales group by OrderMonthYear, ProductCategory\")\nsqlDF.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a22b9588-0e90-4692-adb0-38a78c6799a2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----------+--------------+---------------+\n|SaleAmount|OrderMonthYear|ProductCategory|\n+----------+--------------+---------------+\n|   1152.00|    2010-10-01|    Development|\n|    277.20|    2012-02-01|    Development|\n+----------+--------------+---------------+\nonly showing top 2 rows\n\n+--------------+---------------+------------------+------------------+---------------+---------------+\n|OrderMonthYear|ProductCategory|   sum(SaleAmount)|   avg(SaleAmount)|min(SaleAmount)|max(SaleAmount)|\n+--------------+---------------+------------------+------------------+---------------+---------------+\n|    2009-01-01|     Consulting|          148624.5|           2972.49|        1188.00|         976.50|\n|    2009-01-01|    Development| 438987.1999999997| 3569.001626016258|         103.40|         990.00|\n|    2009-01-01|       Training|          153412.5|           3068.25|        1125.00|         735.00|\n|    2009-02-01|     Consulting|          145953.0|          3648.825|        1039.50|         810.00|\n|    2009-02-01|    Development| 268441.8999999999| 3579.225333333332|         109.20|         990.00|\n|    2009-02-01|       Training|         129846.25| 3509.358108108108|        1175.00|         960.00|\n|    2009-03-01|     Consulting|          196096.5|3162.8467741935483|        1039.50|         891.00|\n|    2009-03-01|    Development|          263179.4|3060.2255813953493|        1008.00|         980.10|\n|    2009-03-01|       Training|         104226.25| 2816.925675675676|        1113.75|         742.50|\n|    2009-04-01|     Consulting|          157069.5|         3926.7375|        1018.50|         855.00|\n|    2009-04-01|    Development|326566.89999999985| 3474.115957446807|        1015.20|        9306.00|\n|    2009-04-01|       Training|          135375.0|            3562.5|        1057.50|         930.00|\n|    2009-05-01|     Consulting|          173158.5|3935.4204545454545|         141.00|         846.00|\n|    2009-05-01|    Development| 364845.2000000001|3316.7745454545466|         107.80|         970.00|\n|    2009-05-01|       Training|          103155.0|3684.1071428571427|        1251.25|         805.00|\n|    2009-06-01|     Consulting|          115530.0|3300.8571428571427|        1008.00|        7203.00|\n|    2009-06-01|    Development|327544.80000000005| 3560.269565217392|        1116.00|        9500.00|\n|    2009-06-01|       Training|         115213.75| 3113.885135135135|         120.00|         857.50|\n|    2009-07-01|     Consulting|          177009.0|         3687.6875|        1008.00|         727.50|\n|    2009-07-01|    Development| 392883.1000000001| 3604.432110091744|         108.90|         960.30|\n+--------------+---------------+------------------+------------------+---------------+---------------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+--------------+---------------+\nSaleAmount|OrderMonthYear|ProductCategory|\n+----------+--------------+---------------+\n   1152.00|    2010-10-01|    Development|\n    277.20|    2012-02-01|    Development|\n+----------+--------------+---------------+\nonly showing top 2 rows\n\n+--------------+---------------+------------------+------------------+---------------+---------------+\nOrderMonthYear|ProductCategory|   sum(SaleAmount)|   avg(SaleAmount)|min(SaleAmount)|max(SaleAmount)|\n+--------------+---------------+------------------+------------------+---------------+---------------+\n    2009-01-01|     Consulting|          148624.5|           2972.49|        1188.00|         976.50|\n    2009-01-01|    Development| 438987.1999999997| 3569.001626016258|         103.40|         990.00|\n    2009-01-01|       Training|          153412.5|           3068.25|        1125.00|         735.00|\n    2009-02-01|     Consulting|          145953.0|          3648.825|        1039.50|         810.00|\n    2009-02-01|    Development| 268441.8999999999| 3579.225333333332|         109.20|         990.00|\n    2009-02-01|       Training|         129846.25| 3509.358108108108|        1175.00|         960.00|\n    2009-03-01|     Consulting|          196096.5|3162.8467741935483|        1039.50|         891.00|\n    2009-03-01|    Development|          263179.4|3060.2255813953493|        1008.00|         980.10|\n    2009-03-01|       Training|         104226.25| 2816.925675675676|        1113.75|         742.50|\n    2009-04-01|     Consulting|          157069.5|         3926.7375|        1018.50|         855.00|\n    2009-04-01|    Development|326566.89999999985| 3474.115957446807|        1015.20|        9306.00|\n    2009-04-01|       Training|          135375.0|            3562.5|        1057.50|         930.00|\n    2009-05-01|     Consulting|          173158.5|3935.4204545454545|         141.00|         846.00|\n    2009-05-01|    Development| 364845.2000000001|3316.7745454545466|         107.80|         970.00|\n    2009-05-01|       Training|          103155.0|3684.1071428571427|        1251.25|         805.00|\n    2009-06-01|     Consulting|          115530.0|3300.8571428571427|        1008.00|        7203.00|\n    2009-06-01|    Development|327544.80000000005| 3560.269565217392|        1116.00|        9500.00|\n    2009-06-01|       Training|         115213.75| 3113.885135135135|         120.00|         857.50|\n    2009-07-01|     Consulting|          177009.0|         3687.6875|        1008.00|         727.50|\n    2009-07-01|    Development| 392883.1000000001| 3604.432110091744|         108.90|         960.30|\n+--------------+---------------+------------------+------------------+---------------+---------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["<pre><b>Q12 Spark API </b>\nUsing movies dataset from databricks samples:\n=> /databricks-datasets/cs110x/ml-20m/data-001/movies.csv\n=> /databricks-datasets/cs110x/ml-20m/data-001/ratings.csv\nRank movies by their popularity i.e., count the ratings given for each movie. \nSort the movies by their number of ratings in ascending order. \nSubmit Python script and the screenshot of terminal‚Äôs output.</pre>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48d45a6f-2193-42bf-bc28-99b6671c1935"}}},{"cell_type":"code","source":["# spark is from the previous example\nsc = spark.sparkContext\npath1 = \"/databricks-datasets/cs110x/ml-20m/data-001/movies.csv\"\npath2 = \"/databricks-datasets/cs110x/ml-20m/data-001/ratings.csv\"\n\ndf1 = spark.read.option(\"header\", \"true\").csv(path1)\ndf2 = spark.read.option(\"header\", \"true\").csv(path2)\n\n#df1.show(2)\n#df2.show(2)\n\ndf1.createOrReplaceTempView(\"Movies\")\ndf2.createOrReplaceTempView(\"Ratings\")\n\n#sqlDF = spark.sql(\"select * from Movies m, Ratings r \" + \"where m.movieId == r.movieId\")\n#sqlDF.show()\n\nspark.sql(\"select m.movieId,title,count(rating) from Movies m, Ratings r \" + \"where m.movieId == r.movieId group by m.movieId,m.title order by count(rating)\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db7f1641-7068-4d92-9030-0af06647d6c8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+--------------------+-------------+\n|movieId|               title|count(rating)|\n+-------+--------------------+-------------+\n|  26863|  Golden Gate (1994)|            1|\n| 110449|Sun Don&#39;t Shine (...|            1|\n| 106702|Strange Case of D...|            1|\n|  98467|Killer Meteors, T...|            1|\n|  89549| A Via L√°ctea (2007)|            1|\n|  83417|Canciones de amor...|            1|\n|  83163|Princess Aurora (...|            1|\n| 117172|         BFFs (2014)|            1|\n|  67316|Children of Lenin...|            1|\n| 119226|Nicholas on Holid...|            1|\n|  95705|       Sasori (2008)|            1|\n| 112764|Last Taboo, The (...|            1|\n| 127005|A Year Along the ...|            1|\n|  66491|     Kimberly (1999)|            1|\n| 123024|Man Made Monster ...|            1|\n| 114909|Coelacanth: The F...|            1|\n| 118864|A Husband of Roun...|            1|\n| 104800|Schwarze Sonne (1...|            1|\n| 106190|  Red Garters (1954)|            1|\n| 102360|Where the Road Me...|            1|\n+-------+--------------------+-------------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+--------------------+-------------+\nmovieId|               title|count(rating)|\n+-------+--------------------+-------------+\n  26863|  Golden Gate (1994)|            1|\n 110449|Sun Don&#39;t Shine (...|            1|\n 106702|Strange Case of D...|            1|\n  98467|Killer Meteors, T...|            1|\n  89549| A Via L√°ctea (2007)|            1|\n  83417|Canciones de amor...|            1|\n  83163|Princess Aurora (...|            1|\n 117172|         BFFs (2014)|            1|\n  67316|Children of Lenin...|            1|\n 119226|Nicholas on Holid...|            1|\n  95705|       Sasori (2008)|            1|\n 112764|Last Taboo, The (...|            1|\n 127005|A Year Along the ...|            1|\n  66491|     Kimberly (1999)|            1|\n 123024|Man Made Monster ...|            1|\n 114909|Coelacanth: The F...|            1|\n 118864|A Husband of Roun...|            1|\n 104800|Schwarze Sonne (1...|            1|\n 106190|  Red Garters (1954)|            1|\n 102360|Where the Road Me...|            1|\n+-------+--------------------+-------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["<pre><b>Q13 Spark API </b>\nFor the ranking movies example, verify your result using SQL with SQLContext or SparkSession. Submit screenshots showing PySpark queries and the first few rows of the result set.</pre>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"23f5bd77-d96c-4d87-b789-e15cf207f378"}}},{"cell_type":"code","source":["results = spark.sql(\"select m.movieId,title,count(rating) from Movies m, Ratings r \" + \n                    \"where m.movieId == r.movieId group by m.movieId,m.title order by count(rating) desc\")\nresults.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"edbd0ced-f655-41d9-a80b-5485d69cfb9b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+--------------------+-------------+\n|movieId|               title|count(rating)|\n+-------+--------------------+-------------+\n|    296| Pulp Fiction (1994)|        67310|\n|    356| Forrest Gump (1994)|        66172|\n|    318|Shawshank Redempt...|        63366|\n|    593|Silence of the La...|        63299|\n|    480|Jurassic Park (1993)|        59715|\n|    260|Star Wars: Episod...|        54502|\n|    110|   Braveheart (1995)|        53769|\n|    589|Terminator 2: Jud...|        52244|\n|   2571|  Matrix, The (1999)|        51334|\n|    527|Schindler&#39;s List ...|        50054|\n|      1|    Toy Story (1995)|        49695|\n|    457|Fugitive, The (1993)|        49581|\n|    150|    Apollo 13 (1995)|        47777|\n|    780|Independence Day ...|        47048|\n|     50|Usual Suspects, T...|        47006|\n|   1210|Star Wars: Episod...|        46839|\n|    592|       Batman (1989)|        46054|\n|   1196|Star Wars: Episod...|        45313|\n|   2858|American Beauty (...|        44987|\n|     32|Twelve Monkeys (a...|        44980|\n+-------+--------------------+-------------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+--------------------+-------------+\nmovieId|               title|count(rating)|\n+-------+--------------------+-------------+\n    296| Pulp Fiction (1994)|        67310|\n    356| Forrest Gump (1994)|        66172|\n    318|Shawshank Redempt...|        63366|\n    593|Silence of the La...|        63299|\n    480|Jurassic Park (1993)|        59715|\n    260|Star Wars: Episod...|        54502|\n    110|   Braveheart (1995)|        53769|\n    589|Terminator 2: Jud...|        52244|\n   2571|  Matrix, The (1999)|        51334|\n    527|Schindler&#39;s List ...|        50054|\n      1|    Toy Story (1995)|        49695|\n    457|Fugitive, The (1993)|        49581|\n    150|    Apollo 13 (1995)|        47777|\n    780|Independence Day ...|        47048|\n     50|Usual Suspects, T...|        47006|\n   1210|Star Wars: Episod...|        46839|\n    592|       Batman (1989)|        46054|\n   1196|Star Wars: Episod...|        45313|\n   2858|American Beauty (...|        44987|\n     32|Twelve Monkeys (a...|        44980|\n+-------+--------------------+-------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DS-610_FinalExam","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1668279129375943}},"nbformat":4,"nbformat_minor":0}
